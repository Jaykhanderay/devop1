{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNOIABRcHbzQ4GZQblJYxGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliots234/devop1/blob/master/ML2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3WYRmkiH98r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "#\n",
        "# Back propagation algorithm realization\n",
        "#\n",
        "#\n",
        "# From\n",
        "# https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b\n",
        "#\n",
        "# The back propagation algorithm begins by comparing the actual value output\n",
        "# by the forward propagation process to the expected value and then moves\n",
        "# backward through the network, slightly adjusting each of the weights in\n",
        "# a direction that reduces the size of the error by a small degree.\n",
        "#\n",
        "# Both forward and back propagation are re-run thousands of times on each\n",
        "# input combination until the network can accurately predict the expected\n",
        "# output of the possible inputs using forward propagation.\n",
        "#\n",
        "# For the XOR problem, 100% of possible data examples are available to use\n",
        "# in the training process. We can therefore expect the trained network to be\n",
        "# 100% accurate in its predictions and there is no need to be concerned with\n",
        "# issues such as bias and variance in the resulting model.\n",
        "#\n",
        "\n",
        "class OneHiddenLayerNetwork:\n",
        "\n",
        "    #\n",
        "    # activation functions\n",
        "    #\n",
        "\n",
        "    @staticmethod\n",
        "    def tang(y):\n",
        "        return np.tanh(y)\n",
        "\n",
        "    @staticmethod\n",
        "    def derivative_tang(y):\n",
        "        return 1.0 - y ** 2\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(y):\n",
        "        return 1 / (1 + np.exp(-y))\n",
        "\n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(y):\n",
        "        return y * (1 - y)\n",
        "\n",
        "    def __init__(self, learning_rate=0.1):\n",
        "\n",
        "        #\n",
        "        # neural network architecture\n",
        "        # simple 2 x 2 x 1 that is enough for XOR example\n",
        "        # input x hidden x output\n",
        "        self.learning_rate = learning_rate\n",
        "        self.output = None\n",
        "\n",
        "        # weights with random values\n",
        "        self.weights = [\n",
        "            np.random.uniform(low=-0.2, high=0.2, size=(2, 2)),  # input layer\n",
        "            np.random.uniform(low=-2, high=2, size=(2, 1))  # hidden layer\n",
        "        ]\n",
        "\n",
        "    def activation(self, activation_type, y):\n",
        "        if activation_type == 'sigmoid':\n",
        "            return self.sigmoid(y)\n",
        "        if activation_type == 'tang':\n",
        "            return self.tang(y)\n",
        "\n",
        "        raise ValueError('Undefined activation function: {}'.format(activation_type))\n",
        "\n",
        "    def derivative_activation(self, activation_type, y):\n",
        "        if activation_type == 'sigmoid':\n",
        "            return self.derivative_sigmoid(y)\n",
        "        if activation_type == 'tang':\n",
        "            return self.derivative_tang(y)\n",
        "\n",
        "        raise ValueError('Undefined derivative activation function: {}'.format(activation_type))\n",
        "\n",
        "    #\n",
        "    # forward pass\n",
        "    # layer by layer\n",
        "    #\n",
        "    def feed_forward_pass(self, x_values):\n",
        "\n",
        "        # forward\n",
        "        input_layer = x_values\n",
        "        hidden_layer = self.activation('tang', np.dot(input_layer, self.weights[0]))\n",
        "        output_layer = self.activation('tang', np.dot(hidden_layer, self.weights[1]))\n",
        "\n",
        "        self.layers = [\n",
        "            input_layer,\n",
        "            hidden_layer,\n",
        "            output_layer\n",
        "        ]\n",
        "\n",
        "        # last layer is an output\n",
        "        return self.layers[2]\n",
        "\n",
        "    #\n",
        "    # back propagation error through the network layers\n",
        "    #\n",
        "    def backward_pass(self, target_output, actual_output):\n",
        "\n",
        "        # divergence of network output\n",
        "        err = (target_output - actual_output)\n",
        "\n",
        "        # backward from output to input layer\n",
        "        # propagate gradients using chain rule\n",
        "        for backward in range(2, 0, -1):\n",
        "            err_delta = err * self.derivative_activation('tang', self.layers[backward])\n",
        "\n",
        "            # update weights using computed gradient\n",
        "            self.weights[backward - 1] += self.learning_rate * np.dot(self.layers[backward - 1].T, err_delta)\n",
        "\n",
        "            # propagate error using updated weights of previous layer\n",
        "            err = np.dot(err_delta, self.weights[backward - 1].T)\n",
        "\n",
        "    def train(self, x_values, target):\n",
        "        self.output = self.feed_forward_pass(x_values)\n",
        "        self.backward_pass(target, self.output)\n",
        "\n",
        "    def predict(self, x_values):\n",
        "        return self.feed_forward_pass(x_values)\n",
        "\n",
        "\n",
        "#\n",
        "# X - XOR dataset data\n",
        "# y = target\n",
        "#\n",
        "X = np.array(([0, 0], [0, 1], [1, 0], [1, 1]), dtype=float)\n",
        "y = np.array(([0], [1], [1], [0]), dtype=float)\n",
        "\n",
        "network = OneHiddenLayerNetwork(learning_rate=0.1)\n",
        "iterations = 5000\n",
        "\n",
        "# training\n",
        "for i in range(iterations):\n",
        "    network.train(X, y)\n",
        "\n",
        "    ten = iterations // 10\n",
        "    if i % ten == 0:\n",
        "        print('-' * 10)\n",
        "        print(\"Iteration number: \" + str(i) + ' / ' +\n",
        "              \"Squared loss: \" + str(np.mean(np.square(y - network.output))))\n",
        "\n",
        "# predict\n",
        "for i in range(len(X)):\n",
        "    print('-' * 10)\n",
        "    print('Input value: ' + str(X[i]))\n",
        "    print('Predicted target: ' + str(network.predict(X[i])))\n",
        "    print('Actual target: ' + str(y[i]))"
      ]
    }
  ]
}